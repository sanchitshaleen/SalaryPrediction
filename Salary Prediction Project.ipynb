{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Testing classification models to predict salaries from the text contained in the job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import classification_report\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "salary = pd.read_csv(\"Train_rev1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Get first 15K rows, and assign an indicator variable for High / Low salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JD = salary[['FullDescription', 'SalaryNormalized']][:15000]\n",
    "\n",
    "mask = np.percentile(JD['SalaryNormalized'], 75)\n",
    "\n",
    "JD.loc[JD['SalaryNormalized'] >= mask,'Class'] = 1\n",
    "JD.loc[JD['Class'] != 1, 'Class'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the training and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 6564, 1.0: 2436})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from patsy import dmatrices\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(JD['FullDescription'], JD['Class'], test_size=0.4, random_state=1)\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####0 is the lower salary, and 1 is the higher\n",
    "#####We have High Salaries to Low Salries in the ratio 1:3, so we have to use a stratified sampling method to train our model well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_df = pd.DataFrame(y_train)\n",
    "\n",
    "y_train_df_0 = y_train_df[y_train_df['Class']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random as rd\n",
    "\n",
    "#we find that there are 2436 observations greater than 75th percentile in the training data. \n",
    "\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "\n",
    "y_train_df_0 = y_train_df[y_train_df['Class']==0]\n",
    "\n",
    "sampleset = random.sample(y_train_df_0.index,2436)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_0 = y_train.ix[sampleset]\n",
    "\n",
    "X_train_0 = X_train.ix[sampleset]\n",
    "\n",
    "y1index = y_train_df[y_train_df['Class']==1].index\n",
    "\n",
    "y_train_1 = y_train.ix[y1index]\n",
    "\n",
    "X_train_1 = X_train.ix[y1index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating the X train & Y trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_1,X_train_0], axis = 0)\n",
    "y_train = pd.concat([y_train_1,y_train_0], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 1), strip_accents='unicode', norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: Multinomial Naive Bayes\n",
      "\n",
      "The precision for this classifier is 0.585229911751\n",
      "The recall for this classifier is 0.793450881612\n",
      "The f1 for this classifier is 0.673616680032\n",
      "The accuracy for this classifier is 0.7965\n",
      "\n",
      "Here is the classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.91      0.80      0.85      4412\n",
      "        1.0       0.59      0.79      0.67      1588\n",
      "\n",
      "avg / total       0.83      0.80      0.80      6000\n",
      "\n",
      "\n",
      "Here is the confusion matrix:\n",
      "[[3519  893]\n",
      " [ 328 1260]]\n"
     ]
    }
   ],
   "source": [
    "X_train_Orig = X_train\n",
    "X_test_Orig = X_test\n",
    "\n",
    "#Unigram, no stop word removal, no lemma\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_nb_predicted = nb_classifier.predict(X_test)\n",
    "\n",
    "print \"MODEL: Multinomial Naive Bayes\\n\"\n",
    "\n",
    "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_nb_predicted))\n",
    "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_nb_predicted))\n",
    "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_nb_predicted))\n",
    "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_nb_predicted))\n",
    "\n",
    "print '\\nHere is the classification report:'\n",
    "print classification_report(y_test, y_nb_predicted)\n",
    "\n",
    "print '\\nHere is the confusion matrix:'\n",
    "print metrics.confusion_matrix(y_test, y_nb_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets lemmatize the corpus and see if our accuracy measures improve or worsen\n",
    "#### Our speculation is that lemmatization will probably decrease the accuracy, as we are bringing each word closer to its root (albeit within context). This is beacuse we are going towards a generalization of each word, hence increasing recall, and negatively affecting precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lemmatize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def LemmaTokenize(text):\n",
    "    return [wnl.lemmatize(t) for t in nltk.word_tokenize(text)]\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, \n",
    " ngram_range=(1, 1), \n",
    " tokenizer = LemmaTokenize, \n",
    " strip_accents='unicode', \n",
    " norm='l2')\n",
    "\n",
    "X_train = X_train_Orig\n",
    "X_test = X_test_Orig\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_nb_predicted = nb_classifier.predict(X_test)\n",
    "\n",
    "print \"MODEL: Multinomial Naive Bayes\\n\"\n",
    "\n",
    "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_nb_predicted))\n",
    "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_nb_predicted))\n",
    "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_nb_predicted))\n",
    "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_nb_predicted))\n",
    "\n",
    "print '\\nHere is the classification report:'\n",
    "print classification_report(y_test, y_nb_predicted)\n",
    "\n",
    "print '\\nHere is the confusion matrix:'\n",
    "print metrics.confusion_matrix(y_test, y_nb_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The output has confirmed our speculation! \n",
    "#### Retaining the non-lemmatized data, and removing stop words we will run the model.\n",
    "##### We expect that removing stop words should give us better results, as the other words in each Job Descriptions will lend more predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: Multinomial Naive Bayes\n",
      "\n",
      "The precision for this classifier is 0.577605826127\n",
      "The recall for this classifier is 0.799118387909\n",
      "The f1 for this classifier is 0.670541611625\n",
      "The accuracy for this classifier is 0.792166666667\n",
      "\n",
      "Here is the classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.92      0.79      0.85      4412\n",
      "        1.0       0.58      0.80      0.67      1588\n",
      "\n",
      "avg / total       0.83      0.79      0.80      6000\n",
      "\n",
      "\n",
      "Here is the confusion matrix:\n",
      "[[3484  928]\n",
      " [ 319 1269]]\n"
     ]
    }
   ],
   "source": [
    "#Removing Stop words & not lemmatizing\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, \n",
    " ngram_range=(1, 1), \n",
    " stop_words = 'english',                             \n",
    " strip_accents='unicode', \n",
    " norm='l2')\n",
    "\n",
    "X_train = X_train_Orig\n",
    "X_test = X_test_Orig\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_nb_predicted = nb_classifier.predict(X_test)\n",
    "\n",
    "print \"MODEL: Multinomial Naive Bayes\\n\"\n",
    "\n",
    "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_nb_predicted))\n",
    "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_nb_predicted))\n",
    "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_nb_predicted))\n",
    "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_nb_predicted))\n",
    "\n",
    "print '\\nHere is the classification report:'\n",
    "print classification_report(y_test, y_nb_predicted)\n",
    "\n",
    "print '\\nHere is the confusion matrix:'\n",
    "print metrics.confusion_matrix(y_test, y_nb_predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We got a marginally lower accuracy after removing stop words. However the three above methods are very close in their \n",
    "####  accuracy measures, thereby not making a huge difference in the appropriateness of any given approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 words (excluding stopwords) that are most indicative of (1) high salary, and (0) low salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 -10.3692114739 00am6\n",
      "0.0 -10.3692114739 07\n",
      "0.0 -10.3692114739 100th\n",
      "0.0 -10.3692114739 10pm6am\n",
      "0.0 -10.3692114739 11am\n",
      "0.0 -10.3692114739 11pm\n",
      "0.0 -10.3692114739 11th\n",
      "0.0 -10.3692114739 1218\n",
      "0.0 -10.3692114739 13th\n",
      "0.0 -10.3692114739 15am\n",
      "\n",
      "1.0 -5.77487589423 experience\n",
      "1.0 -5.91380705657 manager\n",
      "1.0 -5.92144768439 business\n",
      "1.0 -6.06329085478 management\n",
      "1.0 -6.12457163672 home\n",
      "1.0 -6.12667283877 team\n",
      "1.0 -6.15931903738 role\n",
      "1.0 -6.17595273528 development\n",
      "1.0 -6.20834795695 work\n",
      "1.0 -6.22195537794 skills\n"
     ]
    }
   ],
   "source": [
    "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=10):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "\n",
    "    for coef, feat in topn_class1:\n",
    "        print class_labels[0], coef, feat\n",
    "\n",
    "    print\n",
    "\n",
    "    for coef, feat in reversed(topn_class2):\n",
    "        print class_labels[1], coef, feat\n",
    "\n",
    "\n",
    "most_informative_feature_for_binary_classification(vectorizer, nb_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the words like experience, business, manager, management, team, home, role, etc., matter more or appear more frequently in jobs that have a high salary while the low paying jobs have less intuitive words that are more frequent like the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test the model by including the bigram words in our model. This is done by 2 methods. \n",
    "#### Include the bigram words in the model using the vectorizer to generate the tdidf matrix and then run the naive bayes model. \n",
    "We would expect introduction of bigrams to improve our model as they model could be classified by words that appear together frequently than the appears of the unigrams themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: Multinomial Naive Bayes\n",
      "\n",
      "The precision for this classifier is 0.580186583741\n",
      "The recall for this classifier is 0.82241813602\n",
      "The f1 for this classifier is 0.680385517062\n",
      "The accuracy for this classifier is 0.7955\n",
      "\n",
      "Here is the classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.92      0.79      0.85      4412\n",
      "        1.0       0.58      0.82      0.68      1588\n",
      "\n",
      "avg / total       0.83      0.80      0.80      6000\n",
      "\n",
      "\n",
      "Here is the confusion matrix:\n",
      "[[3467  945]\n",
      " [ 282 1306]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, \n",
    " ngram_range=(1, 2),  \n",
    " strip_accents='unicode',\n",
    " lowercase = True,\n",
    " norm='l2')\n",
    "\n",
    "X_train = X_train_Orig\n",
    "X_test = X_test_Orig\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_nb_predicted = nb_classifier.predict(X_test)\n",
    "\n",
    "print \"MODEL: Multinomial Naive Bayes\\n\"\n",
    "\n",
    "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_nb_predicted))\n",
    "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_nb_predicted))\n",
    "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_nb_predicted))\n",
    "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_nb_predicted))\n",
    "\n",
    "print '\\nHere is the classification report:'\n",
    "print classification_report(y_test, y_nb_predicted)\n",
    "\n",
    "print '\\nHere is the confusion matrix:'\n",
    "\n",
    "\n",
    "print metrics.confusion_matrix(y_test, y_nb_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we see that the accuracy of our model has increased by using the bigram taggers and better than the results obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####2. We also method below, where we extract the POS tags of Bigrams to add to the document matrix and use the vectorize on this matrix for classification\n",
    "We would expect the model to perform better than an unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "salary = pd.read_csv(\"Train_rev1.csv\")\n",
    "JD_new = salary[['FullDescription', 'SalaryNormalized']][:15000]\n",
    "\n",
    "mask = np.percentile(JD_new['SalaryNormalized'], 75)\n",
    "\n",
    "JD_new.loc[JD_new['SalaryNormalized'] >= mask,'Class'] = 1\n",
    "JD_new.loc[JD_new['Class'] != 1, 'Class'] = 0\n",
    "\n",
    "\n",
    "def taglist(text):\n",
    "    text = (text.decode('utf-8'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos =  nltk.pos_tag(tokens)\n",
    "    tags = [t for w,t in pos]\n",
    "    bigram_list =  list(nltk.ngrams(tags,2))\n",
    "    listtoappend = [pos1+ '_' + pos2 for pos1, pos2 in bigram_list]\n",
    "    pos_words = ' '.join(listtoappend)\n",
    "    return text +' '+ pos_words\n",
    "\n",
    "JD_new['FullDescription'] = JD_new['FullDescription'][:15000].map(taglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(JD_new['FullDescription'], JD_new['Class'], test_size=0.4, random_state=1)\n",
    "\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "\n",
    "y_train_df_0 = y_train_df[y_train_df['Class']==0]\n",
    "\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "\n",
    "y_train_df_0 = y_train_df[y_train_df['Class']==0]\n",
    "\n",
    "sampleset = random.sample(y_train_df_0.index,2431)\n",
    "\n",
    "y_train_0 = y_train.ix[sampleset]\n",
    "\n",
    "X_train_0 = X_train.ix[sampleset]\n",
    "\n",
    "y1index = y_train_df[y_train_df['Class']==1].index\n",
    "\n",
    "y_train_1 = y_train.ix[y1index]\n",
    "\n",
    "X_train_1 = X_train.ix[y1index]\n",
    "\n",
    "X_train = pd.concat([X_train_1,X_train_0], axis = 0)\n",
    "y_train = pd.concat([y_train_1,y_train_0], axis = 0)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, \n",
    " ngram_range=(1, 1), \n",
    " strip_accents='unicode', \n",
    " lowercase = True,                            \n",
    " norm='l2')\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "y_nb_predicted = nb_classifier.predict(X_test)\n",
    "\n",
    "print \"MODEL: Multinomial Naive Bayes\\n\"\n",
    "\n",
    "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_nb_predicted))\n",
    "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_nb_predicted))\n",
    "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_nb_predicted))\n",
    "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_nb_predicted))\n",
    "\n",
    "print '\\nHere is the classification report:'\n",
    "print classification_report(y_test, y_nb_predicted)\n",
    "\n",
    "print '\\nHere is the confusion matrix:'\n",
    "print metrics.confusion_matrix(y_test, y_nb_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The accuracy obtained for POS tagged Bigrams is slightly less than the one with the bigram tagger in the vectorizer and also lower than the unigram tagger."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
